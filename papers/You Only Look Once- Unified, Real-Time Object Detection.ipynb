{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper name [You Only Look Once: Unified, Real-Time Object Detection](https://pjreddie.com/media/files/papers/yolo_1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**\n",
    "\n",
    "We present YOLO, a new approach to object detection.\n",
    "Prior work on object detection repurposes classifiers to perform\n",
    "detection. Instead, we frame object detection as a regression\n",
    "problem to spatially separated bounding boxes and\n",
    "associated class probabilities. A single neural network predicts\n",
    "bounding boxes and class probabilities directly from\n",
    "full images in one evaluation. Since the whole detection\n",
    "pipeline is a single network, it can be optimized end-to-end\n",
    "directly on detection performance.\n",
    "Our unified architecture is extremely fast. Our base\n",
    "YOLO model processes images in real-time at 45 frames\n",
    "per second. A smaller version of the network, Fast YOLO,\n",
    "processes an astounding 155 frames per second while\n",
    "still achieving double the mAP of other real-time detectors.\n",
    "Compared to state-of-the-art detection systems, YOLO\n",
    "makes more localization errors but is less likely to predict\n",
    "false positives on background. Finally, YOLO learns very\n",
    "general representations of objects. It outperforms other detection\n",
    "methods, including DPM and R-CNN, when generalizing\n",
    "from natural images to other domains like artwork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Previous model**\n",
    "\n",
    "Systems like deformable <b style='color:red'>parts models (DPM)</b> use a sliding window approach where the\n",
    "classifier is run at evenly spaced locations over the entire image [10].\n",
    "More recent approaches like <b style='color:red'>R-CNN</b> use region proposal methods to first generate potential bounding boxes in an image\n",
    "and then run a classifier on these proposed boxes. After\n",
    "classification, post-processing is used to refine the bounding\n",
    "boxes, eliminate duplicate detections, and rescore the\n",
    "boxes based on other objects in the scene [13]. These complex\n",
    "pipelines are slow and hard to optimize because each\n",
    "individual component must be trained separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**you only look once (YOLO)**\n",
    "\n",
    "We reframe object detection as a single regression problem,\n",
    "straight from image pixels to bounding box coordinates\n",
    "and class probabilities. Using our system, you only\n",
    "look once (YOLO) at an image to predict what objects are\n",
    "present and where they are.YOLO is refreshingly simple: see Figure 1. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance. This unified model has several benefits over traditional methods of object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"images/YOLO1/fig1.png\" width=\"400\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**advantage**\n",
    "\n",
    "- First, YOLO is extremely fast. Since we frame detection\n",
    "as a regression problem we don’t need a complex pipeline.\n",
    "We simply run our neural network on a new image at test\n",
    "time to predict detections\n",
    "\n",
    "- Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region\n",
    "proposal-based techniques, YOLO sees the entire image\n",
    "during training and test time so it implicitly encodes contextual\n",
    "information about classes as well as their appearance.\n",
    "Fast R-CNN, a top detection method [14], mistakes background\n",
    "patches in an image for objects because it can’t see\n",
    "the larger context. YOLO makes less than half the number\n",
    "of background errors compared to Fast R-CNN.\n",
    "\n",
    "- Third, YOLO learns generalizable representations of objects.\n",
    "When trained on natural images and tested on artwork,\n",
    "YOLO outperforms top detection methods like DPM\n",
    "and R-CNN by a wide margin. Since YOLO is highly generalizable\n",
    "it is less likely to break down when applied to\n",
    "new domains or unexpected inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it works**\n",
    "\n",
    "Our system divides the input image into an S × S grid.\n",
    "If the center of an object falls into a grid cell, that grid cell\n",
    "is responsible for detecting that object.\n",
    "Each grid cell predicts B bounding boxes and confidence\n",
    "scores for those boxes. These confidence scores reflect how\n",
    "confident the model is that the box contains an object and\n",
    "also how accurate it thinks the box is that it predicts.\n",
    "\n",
    "<p align=\"center\"><img src=\"images/YOLO1/fig2.png\" width=\"400\"><img src=\"images/YOLO1/fig3.png\" width=\"400\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Architecture**\n",
    "<p align=\"center\"><img src=\"images/YOLO1/fig4.png\" width=\"800\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
